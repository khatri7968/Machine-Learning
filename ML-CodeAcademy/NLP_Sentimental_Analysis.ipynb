{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbfdca7",
   "metadata": {},
   "source": [
    "# Strings to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e92dc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 2, 9: 1, 10: 2, 11: 1}\n",
      "{'this': 1, 'is': 2, 'simple': 3, 'text': 4, 'that': 5, 'i': 6, 'have': 7, 'created': 8, 'in': 9, 'a': 10, 'day': 11}\n"
     ]
    }
   ],
   "source": [
    "vocab = {} #maps words to integer representing it\n",
    "word_encoding = 1\n",
    "def bag_of_words(text):\n",
    "    global word_encoding\n",
    "\n",
    "    words = text.lower().split(\" \") #create a list of all of the words in the text\n",
    "    bag = {} #stores all the encoding and their frequency\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        if word in vocab:\n",
    "            encoding = vocab[word] #get encoding from vocab\n",
    "        \n",
    "        else:\n",
    "            vocab[word] = word_encoding\n",
    "            encoding = word_encoding\n",
    "            word_encoding += 1\n",
    "        \n",
    "        if encoding in bag:\n",
    "            bag[encoding] += 1\n",
    "        else:\n",
    "            bag[encoding] = 1\n",
    "    return bag\n",
    "\n",
    "text  = 'this is simple text that i have created created in a a day'\n",
    "bag = bag_of_words(text)\n",
    "print(bag)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8530080",
   "metadata": {},
   "source": [
    "# Sentimental Analysis for Movies review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb0e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_lables) = imdb.load_data(num_words = VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2a358d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     1,    14,    22,    16,\n",
       "          43,   530,   973,  1622,  1385,    65,   458,  4468,    66,\n",
       "        3941,     4,   173,    36,   256,     5,    25,   100,    43,\n",
       "         838,   112,    50,   670, 22665,     9,    35,   480,   284,\n",
       "           5,   150,     4,   172,   112,   167, 21631,   336,   385,\n",
       "          39,     4,   172,  4536,  1111,    17,   546,    38,    13,\n",
       "         447,     4,   192,    50,    16,     6,   147,  2025,    19,\n",
       "          14,    22,     4,  1920,  4613,   469,     4,    22,    71,\n",
       "          87,    12,    16,    43,   530,    38,    76,    15,    13,\n",
       "        1247,     4,    22,    17,   515,    17,    12,    16,   626,\n",
       "          18, 19193,     5,    62,   386,    12,     8,   316,     8,\n",
       "         106,     5,     4,  2223,  5244,    16,   480,    66,  3785,\n",
       "          33,     4,   130,    12,    16,    38,   619,     5,    25,\n",
       "         124,    51,    36,   135,    48,    25,  1415,    33,     6,\n",
       "          22,    12,   215,    28,    77,    52,     5,    14,   407,\n",
       "          16,    82, 10311,     8,     4,   107,   117,  5952,    15,\n",
       "         256,     4, 31050,     7,  3766,     5,   723,    36,    71,\n",
       "          43,   530,   476,    26,   400,   317,    46,     7,     4,\n",
       "       12118,  1029,    13,   104,    88,     4,   381,    15,   297,\n",
       "          98,    32,  2071,    56,    26,   141,     6,   194,  7486,\n",
       "          18,     4,   226,    22,    21,   134,   476,    26,   480,\n",
       "           5,   144,    30,  5535,    18,    51,    36,    28,   224,\n",
       "          92,    25,   104,     4,   226,    65,    16,    38,  1334,\n",
       "          88,    12,    16,   283,     5,    16,  4472,   113,   103,\n",
       "          32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d32ed",
   "metadata": {},
   "source": [
    "***More Preprocessing***\n",
    "If we have a look at some of our loaded in reviews we'll notice that they are different lengths. This is an issue.We cannot pass different length\n",
    "data into our neural network.Therefore we must make each review the same length.To do this we will follow the procedure below:\n",
    "• if the review is greater than 250 words then trim off the extra words\n",
    "• if the review is less than 250 words add the necessary amount of's to make it equal to 250.\n",
    "Luckily for us keras has a function that can do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3106b964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...    19   178    32]\n",
      " [    0     0     0 ...    16   145    95]\n",
      " [    0     0     0 ...     7   129   113]\n",
      " ...\n",
      " [    0     0     0 ...     4  3586 22459]\n",
      " [    0     0     0 ...    12     9    23]\n",
      " [    0     0     0 ...   204   131     9]]\n"
     ]
    }
   ],
   "source": [
    "train_data = pad_sequences(train_data, MAXLEN)\n",
    "test_data = pad_sequences(test_data, MAXLEN)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020504e5",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06c37351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57f89715",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc737a39",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8428c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 73s 115ms/step - loss: 0.4465 - acc: 0.7961 - val_loss: 0.3089 - val_acc: 0.8776\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 39s 62ms/step - loss: 0.2451 - acc: 0.9058 - val_loss: 0.2699 - val_acc: 0.8942\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.1904 - acc: 0.9312 - val_loss: 0.3117 - val_acc: 0.8848\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 46s 74ms/step - loss: 0.1534 - acc: 0.9447 - val_loss: 0.2869 - val_acc: 0.8922\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.1293 - acc: 0.9534 - val_loss: 0.3251 - val_acc: 0.8906\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 47s 74ms/step - loss: 0.1107 - acc: 0.9604 - val_loss: 0.3011 - val_acc: 0.8916\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.0959 - acc: 0.9669 - val_loss: 0.3219 - val_acc: 0.8830\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 47s 75ms/step - loss: 0.0835 - acc: 0.9716 - val_loss: 0.3234 - val_acc: 0.8794\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.0730 - acc: 0.9768 - val_loss: 0.3328 - val_acc: 0.8834\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 50s 80ms/step - loss: 0.0636 - acc: 0.9789 - val_loss: 0.4729 - val_acc: 0.8680\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61699354",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0e63bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27dd4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "# while were at it lets make a decode function\n",
    "\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "    return text[:-1]\n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5956b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 411ms/step\n",
      "[0.8197192]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[0.3272635]\n"
     ]
    }
   ],
   "source": [
    "# Now its time to make a prediction\n",
    "\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1, 250))\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)\n",
    "    print(result[0])\n",
    "    \n",
    "\n",
    "positive_review = \"That movie was awesome! I really loved it and would watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative_review = \"That movie was sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative_review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
